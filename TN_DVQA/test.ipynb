{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for bond_dim = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/qiskit/lib/python3.10/site-packages/cotengra/hyperoptimizers/hyper.py:55: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization. It is recommended to install one of these libraries for higher quality contraction paths.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is (-3.8035316591375707+0j), and waste time 0.00888967514038086\n",
      "Iteration 0: Objective = -3.803532\n",
      "the loss is (-6.0079328820106745+0j), and waste time 0.015073776245117188\n",
      "Iteration 10: Objective = -6.007933\n",
      "the loss is (-6.54871457207554+0j), and waste time 0.011806011199951172\n",
      "Iteration 20: Objective = -6.548715\n",
      "the loss is (-6.859554690832878+0j), and waste time 0.012379169464111328\n",
      "Iteration 30: Objective = -6.859555\n",
      "the loss is (-6.964522991434173+0j), and waste time 0.01265096664428711\n",
      "Iteration 40: Objective = -6.964523\n",
      "the loss is (-7.012533234056632+0j), and waste time 0.011894941329956055\n",
      "Iteration 50: Objective = -7.012533\n",
      "the loss is (-7.024636161018971+0j), and waste time 0.012644767761230469\n",
      "Iteration 60: Objective = -7.024636\n",
      "the loss is (-7.035214685894874+0j), and waste time 0.011266231536865234\n",
      "Iteration 70: Objective = -7.035215\n",
      "the loss is (-7.04211121923786+0j), and waste time 0.009908914566040039\n",
      "Iteration 80: Objective = -7.042111\n",
      "the loss is (-7.045258845775094+0j), and waste time 0.013016223907470703\n",
      "Iteration 90: Objective = -7.045259\n",
      "the loss is (-7.047367630186309+0j), and waste time 0.013283014297485352\n",
      "Iteration 100: Objective = -7.047368\n",
      "the loss is (-7.048859845698344+0j), and waste time 0.01636481285095215\n",
      "Iteration 110: Objective = -7.048860\n",
      "the loss is (-7.049695353597822+0j), and waste time 0.013869762420654297\n",
      "Iteration 120: Objective = -7.049695\n",
      "the loss is (-7.0501311689881465+0j), and waste time 0.013649940490722656\n",
      "Iteration 130: Objective = -7.050131\n",
      "the loss is (-7.050292706958103+0j), and waste time 0.015448808670043945\n",
      "Iteration 140: Objective = -7.050293\n",
      "the loss is (-7.0503462353922846+0j), and waste time 0.015640974044799805\n",
      "Iteration 150: Objective = -7.050346\n",
      "the loss is (-7.050404732758351+0j), and waste time 0.013509035110473633\n",
      "Iteration 160: Objective = -7.050405\n",
      "the loss is (-7.050457270489443+0j), and waste time 0.012804031372070312\n",
      "Iteration 170: Objective = -7.050457\n",
      "the loss is (-7.050497962651624+0j), and waste time 0.014610052108764648\n",
      "Iteration 180: Objective = -7.050498\n",
      "the loss is (-7.050525682294579+0j), and waste time 0.011543035507202148\n",
      "Iteration 190: Objective = -7.050526\n",
      "the loss is (-7.05053961964203+0j), and waste time 0.5322058200836182\n",
      "Optimized Parameters: (array([-1.38078943e+00, -6.40145953e-01,  1.17372962e+00,  4.76015670e-01,\n",
      "        3.84406424e-01,  1.59452388e+00,  1.65789511e+00,  1.36176425e+00,\n",
      "        1.29525645e+00,  3.73584111e-01,  6.79681775e-01,  1.03616956e+00,\n",
      "        4.08706942e-01,  4.22983215e-01,  3.14940758e-01,  1.84591684e-01,\n",
      "       -4.08995706e-01,  6.93578014e-03,  1.49639434e+00, -1.52993396e-02,\n",
      "        3.94134056e-01,  5.09950947e-01,  9.38892711e-01,  1.01262832e+00,\n",
      "        9.36327876e-01, -8.33690444e-03, -6.31923505e-02,  1.41122814e+00,\n",
      "       -2.34494807e-01,  1.00403061e+00,  1.24414462e-02, -9.30123032e-01,\n",
      "       -6.24424649e-02,  2.53780232e-01,  1.56331032e+00, -1.95064029e-01,\n",
      "        3.81186813e-01,  1.57299687e+00,  1.58284708e+00,  1.86621280e-01,\n",
      "        1.39451359e+00,  1.02840729e-01, -3.31486188e-03, -1.16145524e+00,\n",
      "       -1.08333170e-02,  1.54547461e+00,  1.24121954e+00,  2.08038975e+00,\n",
      "        9.22274721e-01,  1.40730907e-02,  1.14586359e+00,  2.55787399e-01,\n",
      "       -1.05650822e-01, -2.61138759e-01, -1.12204388e-02, -2.34840421e-04,\n",
      "        1.71177226e-01,  7.36175392e-01,  3.15081311e-01,  9.81193918e-03,\n",
      "        3.79545667e-03, -1.98508323e-01,  1.53824799e+00,  6.27043526e-01,\n",
      "        1.42929914e-02, -1.47650029e-03,  1.46204383e-01, -8.53024530e-01,\n",
      "       -7.48919259e-01, -1.59641357e+00]), MatrixProductState(tensors=2, indices=3, L=2, max_bond=2))\n",
      "Minimum Objective Value: -7.05053961964203\n",
      "[[-3.8035316591375707, -6.0079328820106745, -6.54871457207554, -6.859554690832878, -6.964522991434173, -7.012533234056632, -7.024636161018971, -7.035214685894874, -7.04211121923786, -7.045258845775094, -7.047367630186309, -7.048859845698344, -7.049695353597822, -7.0501311689881465, -7.050292706958103, -7.0503462353922846, -7.050404732758351, -7.050457270489443, -7.050497962651624, -7.050525682294579, -7.05053961964203]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from DVQE import DVQE\n",
    "\n",
    "\n",
    "def load_hamiltonian_simple(filename=\"hamiltonian.npz\"):\n",
    "    data = np.load(filename, allow_pickle=True)\n",
    "    coefficients = data['coefficients']\n",
    "    pauli_words = data['pauli_words']\n",
    "    return coefficients, pauli_words\n",
    "\n",
    "\n",
    "def divide_into_parts(full_terms, num_parts):\n",
    "    return [tuple(term[i * (len(term) // num_parts):(i + 1) * (len(term) // num_parts)] for i in range(num_parts)) for term in full_terms]\n",
    "\n",
    "# Define parameters for the system\n",
    "num_qubits = 10  # Number of qubits\n",
    "num_parts = num_qubits // 5  # Number of subsystems\n",
    "layer = 6  # Number of layers in the circuit\n",
    "rank = 2  # Initial rank\n",
    "max_rank = 2  # Maximum rank for optimization\n",
    "\n",
    "# Load the Hamiltonian\n",
    "weights, pauli_terms = load_hamiltonian_simple(filename=\"data/hamiltonian_MC_104.npz\")\n",
    "divided_pauli_terms = divide_into_parts(pauli_terms, num_parts)\n",
    "\n",
    "# Dictionary to store loss histories\n",
    "loss_histories = {}\n",
    "\n",
    "\n",
    "bond_dim = 2\n",
    "print(f\"Running for bond_dim = {bond_dim}\")\n",
    "loss_histories[bond_dim] = []  # Initialize list for this bond_dim\n",
    "    \n",
    "# Initialize DVQE instance\n",
    "dvqe = DVQE(num_qubits, num_parts, rank, weights, layer, divided_pauli_terms, bond_dim)\n",
    "\n",
    "np.random.seed(0)\n",
    "# Set initial parameters\n",
    "initial_params = np.random.rand(num_qubits * (layer + 1) + dvqe.num_C)\n",
    "\n",
    "# Optimize\n",
    "optimized_params, min_value = dvqe.optimize(initial_params)\n",
    "\n",
    "# Save the loss history for this run\n",
    "loss_histories[bond_dim].append(dvqe.loss_history)\n",
    "\n",
    "# Print results for this run\n",
    "print(f\"Optimized Parameters: {optimized_params}\")\n",
    "print(f\"Minimum Objective Value: {min_value}\")\n",
    "print(loss_histories[bond_dim])\n",
    "# # Convert keys to strings before saving\n",
    "# np.savez(\"loss_histories.npz\", **{str(key): value for key, value in loss_histories.items()})\n",
    "# print(\"Loss histories saved to 'loss_histories.npz'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
